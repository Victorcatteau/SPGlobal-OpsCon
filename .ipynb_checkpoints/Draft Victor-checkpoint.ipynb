{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from collections import Counter\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "#\n",
    "# Domain specific libraries to handle text\n",
    "#\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12.0, 10.0) # set default size of plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "unhealthy = pd.read_excel('/Users/victor/Documents/GitHub/SP-Global-Ops-Consulting/data/Fund Articles_Unhealthy_New.xlsx')\n",
    "healthy = pd.read_excel('/Users/victor/Documents/GitHub/SP-Global-Ops-Consulting/data/Funds Articles_Healthy_New.xlsx')\n",
    "healthy.drop('Unnamed: 1',axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy['article']=healthy['Healthy Fund Articles']\n",
    "healthy.drop('Healthy Fund Articles',axis=1,inplace=True)\n",
    "unhealthy['article']=unhealthy['Unhealthy Fund Articles']\n",
    "unhealthy.drop('Unhealthy Fund Articles',axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "healthy['label']=1\n",
    "unhealthy['label']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([healthy,unhealthy],axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mitel Returns To Profit In The Second Quarter ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TriplePoint Venture Growth BDC Corp. Announces...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@ M&amp;A wrap: Sycamore, Staples, Leonardo DiCapr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GTIS Partners LP Broadens Business Model; Anno...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@ Cinven to buy Dublin-headquartered Axa Life ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article  label\n",
       "0  Mitel Returns To Profit In The Second Quarter ...      1\n",
       "1  TriplePoint Venture Growth BDC Corp. Announces...      1\n",
       "2  @ M&A wrap: Sycamore, Staples, Leonardo DiCapr...      1\n",
       "3  GTIS Partners LP Broadens Business Model; Anno...      1\n",
       "4  @ Cinven to buy Dublin-headquartered Axa Life ...      1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['article'], y, test_size=0.33, random_state=53)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "punctuation=list(string.punctuation)\n",
    "stop_words=stopwords.words(\"english\")+punctuation+['``',\"''\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lemma_tokenizer(text):\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in word_tokenize(text.replace(\"'\",\" \"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stem_tokenizer(text):\n",
    "    porter_stemmer=PorterStemmer()\n",
    "    return [porter_stemmer.stem(token) for token in word_tokenize(text.replace(\"'\",\" \"))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Count Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "countVectorizer=CountVectorizer(input='content',tokenizer=lemma_tokenizer,stop_words=stop_words)\n",
    "count_train =countVectorizer.fit_transform(X_train.values)\n",
    "count_test =countVectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFIDF Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidfVectorizer=TfidfVectorizer(input=\"content\",tokenizer=lemma_tokenizer,stop_words=stop_words)\n",
    "tfidf_train= tfidfVectorizer.fit_transform(X_train.values)\n",
    "tfidf_test = tfidfVectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83569217160548426"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha=1\n",
    "model=MultinomialNB(alpha)\n",
    "model.fit(count_train,y_train)\n",
    "Y_pred=model.predict(count_test)\n",
    "np.average(Y_pred==y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73109243697478987"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha=1\n",
    "model=MultinomialNB(alpha)\n",
    "model.fit(tfidf_train,y_train)\n",
    "Y_pred=model.predict(tfidf_test)\n",
    "np.average(Y_pred==y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9230429013710747"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(C=1.0)\n",
    "clf.fit(count_train, y_train)\n",
    "Y_pred = clf.predict(count_test)\n",
    "np.average(Y_pred==y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91862007961079173"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(C=1.0)\n",
    "clf.fit(tfidf_train,y_train)\n",
    "Y_pred = clf.predict(tfidf_test)\n",
    "np.average(Y_pred==y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVD-SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "svd = TruncatedSVD(n_components=200)\n",
    "svd.fit(tfidf_train)\n",
    "xtrain_svd = svd.transform(tfidf_train)\n",
    "xtest_svd = svd.transform(tfidf_test)\n",
    "\n",
    "# Scale the data obtained from SVD. Renaming variable to reuse without scaling.\n",
    "scl = preprocessing.StandardScaler()\n",
    "scl.fit(xtrain_svd)\n",
    "xtrain_svd_scl = scl.transform(xtrain_svd)\n",
    "xtest_svd_scl = scl.transform(xtest_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91729323308270672"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SVC(C=1.0)\n",
    "clf.fit(xtrain_svd_scl, y_train)\n",
    "Y_pred = clf.predict(xtest_svd_scl)\n",
    "np.average(Y_pred==y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Optimization of Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grid function compute un bunch of Cross validation with the parameters of the dictionary associated.\n",
    "This function take lot of time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters1 = [{'C': [1, 10, 100], 'kernel': ['linear']},\n",
    "              {'C': [1, 10, 100], 'kernel': ['rbf'], 'gamma': [0.1, 0.3, 0.5, 0.7, 0.9]}]\n",
    "# best C:10 'gamma'=0.9 , 'kernel' ='rbf'\n",
    "parameters2 = [{'C': [1,3,5], 'kernel': ['linear']},\n",
    "        {'C': [2, 5, 7], 'kernel': ['rbf'], 'gamma': [0.85, 0.9, 0.95]},\n",
    "        {'C': [1,3,5], 'kernel': ['poly'], 'degree': [2,3,4,5,6]}]\n",
    "#C=2,gamma=0.85 kernel=rbf\n",
    "grid_search = GridSearchCV(estimator = clf,\n",
    "                           param_grid = parameters2,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 10,\n",
    "                           n_jobs = -1)\n",
    "grid_search = grid_search.fit(tfidf_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters = grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER with spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The folowing code alows to create a matrix where each row give the number of each word 'type' ('PERSON','NORP'...) in the current document.\n",
    "\n",
    "you will need to download the file 'en_core_web_sm'\n",
    "documentation : https://spacy.io/usage/linguistic-features#section-named-entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "NER_needed=['PERSON','NORP','FAC','ORG','GPE','LOC','PRODUCT','EVENT','WORK_OF_ART','LAW','LANGUAGE','DATE','TIME','PERCENT','MONEY','QUANTITY','ORDINAL','CARDINAL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train_NER=[]\n",
    "for i in range (0,len(X_train)):\n",
    "    \n",
    "    if i%100 == 0:\n",
    "        print (i)\n",
    "        \n",
    "    doc = nlp(X_train.iloc[i])    \n",
    "    tokens_NER =[]\n",
    "    for ent in doc.ents :\n",
    "        tokens_NER.append(ent.label_)\n",
    "    s_out_NER=' '.join(tokens_NER)\n",
    "    corpus_train_NER.append(s_out_NER)\n",
    "\n",
    "cv2 =CountVectorizer()\n",
    "count_train_NER = cv2.fit_transform(corpus_train_NER).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concretely the for loop replace on every document each word by his NER  label. Then we use the countVectorize function to count the number of occurence of each label.\n",
    "You can also use the tfidif function.\n",
    "\n",
    "This function takes a lot of time... I tried to amerliorate it by counting directly the label in a loop but actually finding the NER label for every word in the documents take still a lot of time...\n",
    "\n",
    "the next code is the same as previously but for X_test :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_test_NER=[]\n",
    "for i in range (0,len(X_test)):\n",
    "    if i%100 == 0:\n",
    "        print (i)\n",
    "    doc = nlp(X_test.iloc[i])    \n",
    "    tokens_NER =[]\n",
    "    for ent in doc.ents :\n",
    "        label=ent.label_\n",
    "        if label in NER_needed :\n",
    "            tokens_NER.append(ent.label_)\n",
    "    s_out_NER=' '.join(tokens_NER)\n",
    "    corpus_test_NER.append(s_out_NER)\n",
    "\n",
    "\n",
    "\n",
    "count_test_NER = cv2.transform(corpus_test_NER).toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next is a scaler if needed :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "count_scaled_train_NER = sc_X.fit_transform(count_train_NER)\n",
    "count_scaled_test_NER = sc_X.transform(count_test_NER)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
