{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1D Convolutional Neuronal Network Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Model I mostly use for Image recognition, but some papers say that the one-dimension version could be very interesting to NLP Classifier.\n",
    "\n",
    "The main problem of this algorithm is that it takes many calcul to train the network and my computer is not powerful enough to deal with this kind of training...\n",
    "\n",
    "I think that we can get very good results with this algorithm but it will be very hard to configurate it (find the good hyperparameters to avoid overfeeding) and our other algorithm already works well...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Run this algo you will need to download Tonserflow, thead and Keras in your computer.\n",
    "you will also need to download the Glove embeding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.models import model_from_json\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Convolution1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import Flatten\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing Hyperpara:\n",
    "Taille_Data = 9600 # Number of sample considerate for training and test\n",
    "Word_Dimention = 100 # dimension of GLOVE words 50 , 100 , 200\n",
    "Max_Doc_Lenght = 1000  # troncaturate doc with more than 'Max_Doc_Lenght' words \n",
    "Max_NB_Word = 80000 # Only considerate the 10 000 more common words (110 000 actually)\n",
    "r_Split=0.2 # Proportion of test set size\n",
    "\n",
    "# CNN Hyperpara :\n",
    "Nb_Filter = 128\n",
    "Last_Hlayer_Size = 128\n",
    "Batch_Size = 128\n",
    "Nb_Epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read Data\n",
    "dataset_Healthy = pd.read_excel('Funds Articles_Healthy_New.xlsx', dtype={'Name':str}, quoting = 3)\n",
    "dataset_Unhealthy = pd.read_excel('Fund Articles_Unhealthy_New.xlsx', dtype={'Name':str}, quoting = 3)\n",
    "\n",
    "#Rename Columns\n",
    "dataset_Healthy.columns = ['Articles', 'Labels']\n",
    "dataset_Unhealthy.columns = ['Articles', 'Labels']\n",
    "\n",
    "#Create Labels\n",
    "dataset_Healthy['Labels']=1\n",
    "dataset_Unhealthy['Labels']=0\n",
    "\n",
    "#Schuffle\n",
    "dataset_Healthy = dataset_Healthy.sample(frac=1).reset_index(drop=True)\n",
    "dataset_Unhealthy = dataset_Unhealthy.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "#Troncaturate\n",
    "dataset_Healthy = dataset_Healthy[:min(int(Taille_Data/2),len(dataset_Healthy))]\n",
    "dataset_Unhealthy = dataset_Unhealthy[:min(int(Taille_Data/2),len(dataset_Unhealthy))]\n",
    "\n",
    "#Concatenate\n",
    "data = pd.concat([dataset_Healthy,dataset_Unhealthy])\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "Data_train =  Data[:int(Taille_Data*(1-r_Split))].reset_index(drop=True)\n",
    "Data_test =  Data[int(Taille_Data*(1-r_Split)):Size_Data].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tokenizer function\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "stopwords = set(stopwords.words('english'))\n",
    "def my_tokenizer(s):\n",
    "    s = re.sub('[^1-9a-zA-Z]', ' ', s)\n",
    "    s = s.lower() \n",
    "    tokens = nltk.tokenize.word_tokenize(s)\n",
    "    tokens = [t for t in tokens if len(t) > 2] \n",
    "    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens] \n",
    "    tokens = [t for t in tokens if t not in stopwords] \n",
    "    return tokens\n",
    "\n",
    "# Map the Glove\n",
    "word2vec = {}\n",
    "with open('glove.6B.'+str(Word_Dimention)+'d.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vec = np.asarray(values[1:], dtype='float32')\n",
    "        word2vec[word] = vec\n",
    "\n",
    "#Create corpus an tokenize\n",
    "corpus_train=[]\n",
    "for i in range (0,int(min(Size_Data,len(Data))*(1-r_split))):\n",
    "    tokens = my_tokenizer(data_train['Articles'][i])\n",
    "    tokens = ' '.join(tokens)\n",
    "    corpus_train.append (tokens)\n",
    "\n",
    "#Map the token and build input\n",
    "tokenizer_train = Tokenizer(num_words=Max_NB_Word)\n",
    "tokenizer_train.fit_on_texts(corpus_train)\n",
    "sequences_train = tokenizer_train.texts_to_sequences(corpus_train)\n",
    "word_index_train = tokenizer_train.word_index\n",
    "\n",
    "#Create en embeding matrix\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, Word_Dimention))\n",
    "for word, i in word_index_train.items():\n",
    "    embedding_vector = word2vec.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "#Create train sets\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "X_train = pad_sequences(sequences_train, maxlen=Max_Doc_Lenght)\n",
    "y_train = data_train.iloc[:, 1].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Neuronal Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()\n",
    "\n",
    "classifier.add(Embedding(len(word_index) + 1, Word_Dimention, weights=[embedding_matrix], input_length = Max_Doc_Lenght, trainable=False))\n",
    "\n",
    "classifier.add(Convolution1D(Nb_Filter, 5, activation = 'relu'))\n",
    "classifier.add(MaxPooling1D(pool_size = 5))\n",
    "\n",
    "classifier.add(Convolution1D(Nb_Filter, 5, activation = 'relu'))\n",
    "classifier.add(MaxPooling1D(pool_size = 5))\n",
    "\n",
    "classifier.add(Convolution1D(Nb_Filter, 5, activation = 'relu'))\n",
    "classifier.add(MaxPooling1D(pool_size = 35))\n",
    "\n",
    "classifier.add(Flatten())\n",
    "\n",
    "classifier.add(Dense(output_dim = Last_Hlayer_Size , activation = 'relu'))\n",
    "classifier.add(Dense(output_dim = 1, activation = 'sigmoid'))\n",
    "\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "classifier.fit(X_train, y_train, epochs=Nb_Epochs, batch_size=Batch_Size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build test feature\n",
    "corpus_test=[]\n",
    "for i in range (0,int(min(Size_Data,len(Data))*(r_split))):\n",
    "    tokens = my_tokenizer(data_test['Articles'][i])\n",
    "    tokens = ' '.join(tokens)\n",
    "    corpus_test.append (tokens)\n",
    "\n",
    "tokenizer_test = Tokenizer(num_words=Max_NB_Word)\n",
    "tokenizer_test.fit_on_texts(corpus_test)\n",
    "sequences_test = tokenizer_test.texts_to_sequences(corpus_test)\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "X_test = pad_sequences(sequences_test, maxlen=Max_Doc_Lenght)\n",
    "y_test = data_test.iloc[:, 1].values\n",
    "\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save/Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_json = classifier.to_json()\n",
    "with open(\"classifier.json\", \"w\") as json_file:\n",
    "    json_file.write(classifier_json)\n",
    "    # serialize weights to HDF5\n",
    "    classifier.save_weights(\"classifier.h5\")\n",
    "    print(\"Saved classifier to disk\")\n",
    " \n",
    "    \n",
    "json_file = open('classifier.json', 'r')\n",
    "loaded_name_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_name = model_from_json(loaded_name_json)\n",
    "loaded_name.load_weights(\"classifier.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
