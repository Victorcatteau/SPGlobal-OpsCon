{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning CNN 1D classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "don't try to load it on your computer if you don' t have a big gpu \n",
    "You will also need to download tonsorflow and keras ! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.models import model_from_json\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Convolution1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import Flatten\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing Hyperpara:\n",
    "Word_Dimention = 100 # dimension of GLOVE words 50 , 100 , 200\n",
    "Max_Doc_Lenght = 1000  # troncaturate doc with more than 'Max_Doc_Lenght' words \n",
    "Max_NB_Word = 100000 # Only considerate the 10 000 more common words (110 000 actually)\n",
    "r_Split=0.1 # Proportion of test set size\n",
    "\n",
    "# CNN Hyperpara :\n",
    "Nb_Filter = 128\n",
    "Last_Hlayer_Size = 128\n",
    "Batch_Size = 128\n",
    "Nb_Epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read Data\n",
    "dataset_Healthy = pd.read_excel('Funds Articles_Healthy_New.xlsx', dtype={'Name':str}, quoting = 3)\n",
    "dataset_Unhealthy = pd.read_excel('Fund Articles_Unhealthy_New.xlsx', dtype={'Name':str}, quoting = 3)\n",
    "\n",
    "#Rename Columns\n",
    "dataset_Healthy.columns = ['Articles', 'Labels']\n",
    "dataset_Unhealthy.columns = ['Articles', 'Labels']\n",
    "\n",
    "#Create Labels\n",
    "dataset_Healthy['Labels']=1\n",
    "dataset_Unhealthy['Labels']=0\n",
    "\n",
    "\n",
    "#Concatenate\n",
    "data = pd.concat([dataset_Healthy,dataset_Unhealthy])\n",
    "\n",
    "y = data.iloc[:, 1].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['Articles'], y, test_size = r_Split,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the Glove\n",
    "word2vec = {}\n",
    "with open('glove.6B.'+str(Word_Dimention)+'d.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vec = np.asarray(values[1:], dtype='float32')\n",
    "        word2vec[word] = vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map tokens and build input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=Max_NB_Word)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Embeding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, Word_Dimention))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = word2vec.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(sequences_train, maxlen=Max_Doc_Lenght)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()\n",
    "\n",
    "classifier.add(Embedding(len(word_index) + 1, Word_Dimention, weights=[embedding_matrix], input_length = Max_Doc_Lenght, trainable=False))\n",
    "\n",
    "classifier.add(Convolution1D(Nb_Filter, 5, activation = 'relu'))\n",
    "classifier.add(MaxPooling1D(pool_size = 5))\n",
    "\n",
    "classifier.add(Convolution1D(Nb_Filter, 5, activation = 'relu'))\n",
    "classifier.add(MaxPooling1D(pool_size = 5))\n",
    "\n",
    "classifier.add(Convolution1D(Nb_Filter, 5, activation = 'relu'))\n",
    "classifier.add(MaxPooling1D(pool_size = 35))\n",
    "\n",
    "classifier.add(Flatten())\n",
    "\n",
    "classifier.add(Dense(output_dim = Last_Hlayer_Size , activation = 'relu'))\n",
    "classifier.add(Dense(output_dim = 1, activation = 'sigmoid'))\n",
    "\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(X_train, y_train, epochs=Nb_Epochs, batch_size=Batch_Size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=Max_Doc_Lenght)\n",
    "\n",
    "scores = classifier.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (classifier.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "\n",
    "classifier_json = classifier.to_json()\n",
    "with open(\"classifierCNN.json\", \"w\") as json_file:\n",
    "    json_file.write(classifier_json)\n",
    "    classifier.save_weights(\"classifierCNN.h5\")\n",
    "    print(\"Saved classifier to disk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
