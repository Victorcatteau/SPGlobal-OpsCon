{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier with NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data et library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import nltk\n",
    "\n",
    "\n",
    "\n",
    "#Import Data\n",
    "unhealthy = pd.read_excel('C:/Users/Alu/Desktop/S and P Global/Fund Articles_Unhealthy_New.xlsx')\n",
    "healthy = pd.read_excel('C:/Users/Alu/Desktop/S and P Global/Funds Articles_Healthy_New.xlsx')\n",
    "healthy.drop('Unnamed: 1',axis=1,inplace=True)\n",
    "\n",
    "\n",
    "#Rename Columns\n",
    "healthy['article']=healthy['Healthy Fund Articles']\n",
    "healthy.drop('Healthy Fund Articles',axis=1,inplace=True)\n",
    "unhealthy['article']=unhealthy['Unhealthy Fund Articles']\n",
    "unhealthy.drop('Unhealthy Fund Articles',axis=1,inplace=True)\n",
    "\n",
    "#Create Dependant variable\n",
    "healthy['label']=1\n",
    "unhealthy['label']=0\n",
    "\n",
    "#Concat\n",
    "df = pd.concat([healthy,unhealthy],axis=0,sort=False).reset_index(drop=True)\n",
    "\n",
    "#Create Dependant variable vector\n",
    "y = df['label']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['article'], y, test_size=0.10,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create corpus of NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you already have the file download skit this part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "NER_needed=['PERSON','NORP','FAC','ORG','GPE','LOC','PRODUCT','EVENT','WORK_OF_ART','LAW','LANGUAGE','DATE','TIME','PERCENT','MONEY','QUANTITY','ORDINAL','CARDINAL']\n",
    "\n",
    "\n",
    "corpus_train_NER=[]\n",
    "for i in range (0,len(X_train)):\n",
    "    if i%100 == 0:\n",
    "        print (i)\n",
    "    doc = nlp(X_train.iloc[i])\n",
    "    tokens_NER =[]\n",
    "    for ent in doc.ents :\n",
    "        tokens_NER.append(ent.label_)\n",
    "    s_out_NER=' '.join(tokens_NER)\n",
    "    corpus_train_NER.append(s_out_NER)\n",
    "corpus_train_NER=np.array(corpus_train_NER)\n",
    "\n",
    "np.save('corpus_train_NER', corpus_train_NER)\n",
    "\n",
    "corpus_test_NER=[]\n",
    "for i in range (0,len(X_test)):\n",
    "    if i%100 == 0:\n",
    "        print (i)\n",
    "    doc = nlp(X_test.iloc[i])  \n",
    "    tokens_NER =[]\n",
    "    for ent in doc.ents :\n",
    "        label=ent.label_\n",
    "        if label in NER_needed :\n",
    "            tokens_NER.append(ent.label_)\n",
    "    s_out_NER=' '.join(tokens_NER)\n",
    "    corpus_test_NER.append(s_out_NER)\n",
    "corpus_test_NER=np.array(corpus_test_NER)\n",
    "\n",
    "np.save('corpus_test_NER', corpus_test_NER)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Bago of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train_NER = np.load('corpus_train_NER')\n",
    "corpus_test_NER = np.load('corpus_test_NER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2 =CountVectorizer()\n",
    "count_train_NER = cv2.fit_transform(corpus_train_NER).toarray()\n",
    "count_test_NER = cv2.fit_transform(corpus_test_NER).toarray()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "count_scaled_train_NER = sc_X.fit_transform(count_train_NER)\n",
    "count_scaled_test_NER = sc_X.transform(count_test_NER)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifierSVC_NER = SVC(kernel = 'linear')\n",
    "classifierSVC_NER.fit(count_scaled_train_NER, y_train)\n",
    "y_predSVC_NER = classifierSVC_NER.predict(count_scaled_test_NER)\n",
    "np.average(y_predSVC_NER==y_test)\n",
    "\n",
    "\n",
    "classifierRF_NER_count = RandomForestClassifier(n_estimators = 30, criterion = 'entropy')\n",
    "classifierRF_NER_count.fit(count_scaled_train_NER, y_train)\n",
    "y_predRF_NER_count = classifierRF_NER_count.predict(count_scaled_test_NER)\n",
    "\n",
    "cm =  confusion_matrix(y_test,y_predRF_NER_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = [{'C': [1, 10, 100], 'kernel': ['rbf'], 'gamma': [0.1, 0.3, 0.5, 0.7, 0.9]},\n",
    "              {'C': [1,3,5], 'kernel': ['poly'], 'degree': [2,3,4,5,6]}]\n",
    "parameters = [{'n_estimators': [10, 20, 30], 'criterion': ['gini','entropy']}]\n",
    "grid_search = GridSearchCV(estimator = classifierSVC_NER,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 10,\n",
    "                           n_jobs = -1)\n",
    "grid_search = grid_search.fit(count_scaled_train_NER, y_train)\n",
    "best_accuracy = grid_search.best_score_\n",
    "best_parameters = grid_search.best_params_\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
