{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier First Version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These is a very detailed explanation of a basic NLP classifier if you have any questions don't hesitate to send it to me !!!\n",
    "\n",
    "I put here the first version of Classifier. It use basic ML classifier from the sklearn library. This code can be used as the basic structure for the classifier and each part of the algorithm should of course be ameliorate in order to get the best accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I- Import basic library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I always import these basic libraries at the begining of a program just beacause it is likely that they will be used in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Numpy__ is mostly used to manage lists and matrix efficiently. It is also used for mathematical functions\n",
    "\n",
    "__pandas__ is used to handle the dataset. In this algorithm it will only help us to import the excel data to python as dataframe\n",
    "\n",
    "__nltk__ is used to pre-process the data. You will see later what specific object of the library we will use\n",
    "\n",
    "__sklearn__ is the library that contain the optimized classifier algorithms. You will see later how we use these objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II- Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we import the data from Excel to Python dataframe thanks to pandas and we change the shape of the data in order to create a convenient input for the rest of the algorithm.\n",
    "\n",
    "for the next two lines you will need to adapte the directory of the data in your computer ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_Healthy = pd.read_excel(r'C:\\Users\\Alu\\Desktop\\S and P Global\\Funds Articles_Healthy_New.xlsx', dtype={'Name':str}, quoting = 3)\n",
    "dataset_Unhealthy = pd.read_excel(r'C:\\Users\\Alu\\Desktop\\S and P Global\\Fund Articles_Unhealthy_New.xlsx', dtype={'Name':str}, quoting = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two lines use the function read_excel of the pandas library to import the data from a excel file in your workingdirectory file to a dataframe.\n",
    "To dataframes are created: One for the healthy documents and one for the unhealthy documents.\n",
    "\n",
    "The parameter dtype is used to tell the function to consider the values in the excel as string (a string is a chain of character: 'i am a cat' this is the way python deal with text) \n",
    "The parameter quoting set as 3 just allows to consider quotes (\") as a normal character. Indeed, because string is delimited by quotes, if the algorithm doesn't consider (\") as normal characters it can cause problems if there is quotes in the data.\n",
    "\n",
    "If you want more information about the different parameters of a function you just have to google the name of the function and documentation (for instance: pandas.read_excel documentation) and the first link should give you all the information you need.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_Healthy.columns = ['Articles', 'Labels']\n",
    "dataset_Unhealthy.columns = ['Articles', 'Labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I rename the two columns of the dataframe\n",
    "A dataframe is big table. This is a different object than a simple python table (list of lists) or a numpy table (array) but it works almost the same way. They are most efficient to manage a big set a data but don't worry if you are not comfortable with dataframes we will soon change the type of the data to a numpy arrays (which work exactly as a list of lists).\n",
    "\n",
    "The first column is Article. Each line of this column contains one article in type string.\n",
    "The second column is Label. This column is empty for now, but it will contain 1 if the article on the same line is Healthy and 0 otherwise\n",
    "The next code fills these last columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_Healthy['Labels']=1\n",
    "dataset_Unhealthy['Labels']=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The label column of the healthy dataframe is fill with 1\n",
    "the label column of the Unhealthy dataframe is fill with 0\n",
    "\n",
    "The next lines allow to set the total number documents we will use in the rest of the algorithm. Of course, to get the best accuracy we should make the next ML algorithm learn on the most of data we can. But for the purpose of building the algorithm it is good to have the possibility to set the number of documents considerate manually.\n",
    "\n",
    "So, we will cut the two dataframe (only take the first rows = truncate) to have the total of number of documents we want.\n",
    "But if the documents have a specific order in the excel the selection we will apply can bias the learning of the future algorithm so before truncating the two dataframes we need to shuffle the rows (mix the rows) of these dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Schuffle the dataframes\n",
    "dataset_Healthy = dataset_Healthy.sample(frac=1).reset_index(drop=True)\n",
    "dataset_Unhealthy = dataset_Unhealthy.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "#set the Size of the data as parameter\n",
    "Size_Data = 10000\n",
    "\n",
    "#Troncaturate the dataframes\n",
    "dataset_Healthy = dataset_Healthy[:int(Size_Data/2)]\n",
    "dataset_Unhealthy = dataset_Unhealthy[:int(Size_Data/2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first two lines allow to Shuffle the dataframes. You don't have to care about the parameters. If you want more information about this information google it and find the documentation\n",
    "\n",
    "The last two lines truncate the dataframe (take the Size_Data/2  first row of the data ). \n",
    "\n",
    "Note here that I take the same number of Healthy and Unhealthy documents. Actually, I don't know if training the ML algorithm on a data which have more Healthy document than Unhealthy documents have an impact on the learning algorithm. I don't think that it changes anything, but it was easier for me to truncate like that in a first time.\n",
    "\n",
    "Then, we want one dataframe with the Size_data number of healthy and unhealthy documents in a random order.\n",
    "So, the next lines concatenate the two previous dataframe and shuffle this dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate\n",
    "Data = pd.concat([dataset_Healthy,dataset_Unhealthy])\n",
    "\n",
    "#Schuffle\n",
    "Data = Data.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we split the dataset in a training set and a test set :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_split = 0.2\n",
    "\n",
    "Data_train =  Data[:int(Size_Data*(1-r_split))].reset_index(drop=True)\n",
    "Data_test =  Data[int(Size_Data*(1-r_split)):Size_Data].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "r_split is the proportion of data in the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III- Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will pre-process the training data. The pre-processing phase have three major parts. \n",
    "- We want to tokenize the documents, \n",
    "- we want to create the matrix of feature that we will use as input of the classifier algorithm, \n",
    "- finally, we will scale the matrix of feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A- Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize mean to clean the documents in order to extract the most relevant part of each documents\n",
    "\n",
    "So we will Create a function named tokenizer that takes as input a document (as type string) and return the same document but cleaned (the output is also a string).\n",
    "Here is the function, I explain each line just after the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the re library\n",
    "import re\n",
    "\n",
    "#Import object form nltk \n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# create the function\n",
    "def my_tokenizerI(s):\n",
    "    s = re.sub('[^1-9a-zA-Z]', ' ', s)\n",
    "    s = s.lower() \n",
    "    tokens = nltk.tokenize.word_tokenize(s)\n",
    "    tokens = [t for t in tokens if len(t) > 2] \n",
    "    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens] \n",
    "    tokens = [t for t in tokens if t not in stopwords]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let focus on the function:\n",
    "\n",
    "The input is s, a document of type string (--> s= 'even if his wife is vegetarian, the CEO of JP Morgan loves Chicken Gyros...')\n",
    "\n",
    "__Line 1__ : _s = re.sub('[^1-9a-zA-Z]', ' ', s)_\n",
    "This line removes the characters that are not numbers or letters and replace them with a space:\n",
    "To do so it uses the sub function of the re library that's why I import the library re before the function.\n",
    "s = 'even if his wife is vegetarian the CEO of JP Morgan loves Chicken Gyros '\n",
    "\n",
    "__Line 2__ : _s = s.lower()_\n",
    "This line put all characters in lowercase\n",
    "s = 'even if his wife is vegetarian  the ceo of jp morgan loves chicken gyros '\n",
    "\n",
    "__Line 3__ : _tokens = nltk.tokenize.word_tokenize(s))_\n",
    "This line uses the tokenize function of the nltk library. It transforms the string s in a list of token. A token is simply a unique word of type string:\n",
    "tokens = ['even','if','his','wife','is','vegetarian','the','ceo','of','jp','morgan','loves','chicken','gyros']\n",
    "\n",
    "__Line 4__ : _tokens = [t for t in tokens if len(t) > 2]_\n",
    "This line simply removes all words that have less than 2 letters:\n",
    "tokens = ['even','his','wife','vegetarian','the','ceo','morgan','loves','chicken','gyros']\n",
    "\n",
    "__Line 5__ : _tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens]_\n",
    "This line lemmatizes all the token of the list tokens. It means that it take the roots of the word (working -> work ; loves -> love )\n",
    "I use an object of the nltk library, thatâ€™s why, before the function, I had to import the object from the nltk library and then create an object.\n",
    "tokens = ['even','his','wife','vegetarian','the','ceo','morgan','love','chicken','gyro']\n",
    "\n",
    "__Line 6__ : _tokens = [t for t in tokens if t not in stopwords]_\n",
    "This line removes the words that don't have a really meaning. To do so we need to import the stopwords list. This is simply a list of useless word. There is many lists like that on internet but i choose the one from nltk library. You can see that I imported this list just before the function.\n",
    "tokens = ['even','wife','vegetarian','ceo','morgan','love','chicken','gyro']\n",
    "\n",
    "__Line 7__ : _s = ' '.join(tokens)_\n",
    "This last line transforms the list of token in the output string\n",
    "s = 'even wife vegetarian ceo morgan love chicken gyro'\n",
    "\n",
    "And finally, the output is the cleaned string s\n",
    "\n",
    "Of course, this Tokenizer should be improved, there is a lot of to do here that have a great impact on the result. For instance, at the beginning I remove all the numbers of the documents, When I tried not to remove the numbers, the final result has been improved by 1% accuracy!!\n",
    "\n",
    "Let see now how this function is used, and how to prepare the input of our classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B- Create the matrix of feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is a bit more complicated. Our goal is to create a matrix that our classifier algorithm can understand.\n",
    "\n",
    "Indeed, ML algorithm are based on mathematical models and therefor they only understand numbers. They don't understand the strings in our dataframe. So, we have to find a way to translate our cleaned text into matrix of number.\n",
    "\n",
    "There is two ways to do so:\n",
    "\n",
    "__1 ---> bag of word:__\n",
    "    Imagine you make the list of all the words in all the documents of the data, this set of word is named the vocabulary.\n",
    "    Let say that the size of the vocabulary is 100 000 and the number of documents you use for this algorithm is 10 000\n",
    "    the bag of word matrix A is the 10 000 X 100 000 sized matrix where each line represents a document and each column\n",
    "    represent a word of the vocabulary. The coefficient of the line i and the column j in A is the number of times the word \n",
    "    j appears in the document i.\n",
    "    \n",
    "__1 ---> tfidf :__\n",
    "    This is almost the same thing. This is also a 10 000 X 100 000 sized matrix where each lines represent a document and each\n",
    "    columns represent a word but this time, the coefficient of the line i and the column j is the frequency of appearance of the\n",
    "    word i present in the document j.\n",
    "    \n",
    "to summarize these two methods, allow to change a set of documents into a huge matrix. This matrix will be use as the input of our ML algorithm.\n",
    "\n",
    "The sklearn will provide us the right objects to create these matrices it will be very easy you will see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a clean corpus\n",
    "corpus_train =[]\n",
    "for i in range (0,int(Size_Data*(1-r_split))):\n",
    "    s = my_tokenizerI(Data_train['Articles'][i])\n",
    "    corpus_train.append(s)\n",
    "\n",
    "Max_NB_Word = 100000\n",
    "\n",
    "# Crete the matrix a purpose\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf=TfidfVectorizer(max_features=Max_NB_Word)\n",
    "X_train=tfidf.fit_transform(corpus_train).toarray()\n",
    "\n",
    "#from sklearn.feature_extraction.text import CountVectorizer \n",
    "#cv = CountVectorizer(max_features=Max_NB_Word)\n",
    "#X_train = cv.fit_transform(corpus_train).toarray()\n",
    "\n",
    "y_train = Data_train.iloc[:, 1].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I said we will use a sklearn object to create the matrix of feature. However, this object requires as input a list of string. \n",
    "\n",
    "So before creating the matrix of feature we will create this list. we initialise this list (named corpus) as an empty list. \n",
    "then we loop on each rows of our dataframe and at each iteration we add to the corpus the document of the current row but tokenized with our previous tokenizer function.\n",
    "At the end of the loop (it can take 5 to 10 minutes...) we have the list of all our tokenized documents (in type string).\n",
    "\n",
    "Then we are ready to create the matrix of feature. \n",
    "we import the TfidfVectorizer object from the skleanr library\n",
    "then you initialize the object. It takes one important parameter: 'max_features=' allow to choose the maximal length of your vocabulary. So, if you set this parameter to Max_NB_Word = 100000 the object will only consider the 100 000 most frequent words. It is also a very important parameter it allows to reduce the noise of the data (remove useless words that have bad effect on the result).\n",
    "finally, you use the method fit_transform to the corpus. This method return the huge matrix we want!  \n",
    "the .toarray() allows to get an array matrix which is more efficient than list of list...\n",
    "\n",
    "As you can see I put the code for the two methods. After some tests it seems that the tfidf method give better results, so you can forget the last three line of code!\n",
    "\n",
    "We also create our vector of dependant variable y_train : y_train[i]=1 if the document in the row i of X_train is healthy and y_train[i]=0 otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D- Scale the Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before fitting the model, we need a last small step: scaling the matrix\n",
    "\n",
    "what does that mean? Actually, this is not necessary in our case, but this is always a good thing to do and it doesn't take lot of work.\n",
    "\n",
    "Imagine in a completely other situation you have a data from a bank and you try to classify the customers. Your matrix of feature can have a column that give the income of a given customer and, in another column, you can have his credit score. a normal value for income is around 100 000 and a credit score is around 0.8. If you don't put these two attributes to the same scale the classifier will think that the income is a more important value to classify just because the value is bigger... So, in order to avoid this miss understanding you should provide to your classifier features that have approximatively the same kind of value. So before fitting the model you make an operation on each column to reduce for instance the income to values between -1 and 1.\n",
    "You only have to subtract the mean of the incomes of all the customers to the actual value and divide by the same mean.\n",
    "\n",
    "As I said this is not necessary in our case because all the features are frequency of appearance and therefore all the feature of a given columns are already on the same scale. However, if you use the bag of word method the scaling become necessary (excepte for lineare model such as Logistic regression that don't need to have scaled input ).\n",
    "\n",
    "Overall I think this is better to forget this scaling part. It is very long in term of calculation and we can avoid it by using tfidf method or the logistic regression model...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of this part it is important to see that we only deal with the train data. We have fited all our objects (tfidf and scaler) on the training set. Indeed we used the function fit_transform that make the following to operations:\n",
    "\n",
    "__first__ fit the object to the train data : for the scaler it mean that the object memorise the mean of each columns of the training set.\n",
    "\n",
    "__second__ transform the object : it makes the appropiate transformation according to the previous fit: for instance, for the scaler the transforme function use the list of the mean of each column calculated in the fit method and use it to scale each columm (by substracting the mean of the colone to each coeficient of the colone and then divided by this same mean)\n",
    "\n",
    "This is important to understand that, if we prepared the data with these objects before applying our model, we would need to adapte our test set the same way before testing them on the model. So the object we just create makes part of the model and we will need them to transform our test set in the test phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV- Fit the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will see how to fit a model to our data. You will see thanks to our previous work and the sklearn library this is really easy.\n",
    "\n",
    "In the code I put three different models:\n",
    "\n",
    "- logistic Regression\n",
    "- Naive Bayes\n",
    "- Random Forest\n",
    "- SVM\n",
    "\n",
    "The Math behind each model is a bit complicated and I don't think that it is really important for our project... So I won't explain it here but if you want a quick theorical explanation just ask!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic Regression \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifierNB = GaussianNB()\n",
    "classifierNB.fit(X_train, y_train)\n",
    "\n",
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifierRF = RandomForestClassifier(n_estimators = 20, criterion = 'entropy')\n",
    "classifierRF.fit(X_train, y_train)\n",
    "\n",
    "# Kernel SVM \n",
    "from sklearn.svm import SVC\n",
    "classifierSVC = SVC(kernel = 'rbf')\n",
    "classifierSVC.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One again three lines:\n",
    "\n",
    "import the object,\n",
    "\n",
    "initialise the object,\n",
    "\n",
    "fit the object!\n",
    "\n",
    "And after several minutes of processing you have your classifier ready and fitted !!!\n",
    "We will see later how to use it and how to evaluate it\n",
    "\n",
    "If you look in the documentation of sklearn and look for the different model, you will find that there is a lot of parameters we can add to our classifier to improve them. I think we should focus more on the random forest method, but I didn't try to customize the other methods. \n",
    "\n",
    "Now we have fitted our classifier let see how to evaluate the result!\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V- Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I said previously before testing the test set we should adapt the data set to the model.\n",
    "\n",
    "to do so we need to :\n",
    "- Create the matrix of feature X_test the same way as we created X_train\n",
    "- Scale X_test the same way as we scaled X_train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Corpus test \n",
    "corpus_test =[]\n",
    "for i in range (0,int(Size_Data*(r_split))):\n",
    "    Tokens = my_tokenizerI(Data_test['Articles'][i])\n",
    "    corpus_test.append(Tokens)\n",
    "\n",
    "#Matrix of feature\n",
    "X_test=tfidf.transform(corpus_test).toarray()\n",
    "#X_test=cv.transform(corpus_test).toarray()\n",
    "\n",
    "#Vector of dependant variables\n",
    "y_test = Data_test.iloc[:, 1].values\n",
    "\n",
    "#Scale\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to observe that we don't create new object, we use the one created for preprocessing the training data. Actually we only transforme the test set the same way we transformed the training set ! thats why we use the \"transform\" method of the different object and not the fit_transform method as previously. The object should stay fited to the training set !! \n",
    "\n",
    "\n",
    "Now let see how to make a prediction :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifierRF.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You just have to us the predict function on the classifier and it will return the vector of 1 and 0 that predict if the documents of the test set or healthy or unhealthy.\n",
    "so you can compare y_pred and y_test and hopefully they are similar!!\n",
    "\n",
    "the confusion matrix is a matrix that allow to make this comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[164,  40],\n",
       "       [ 21, 175]], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cm is 2X2 matrix \n",
    "- cm[0][0] = number of unhealthy documents well predicted\n",
    "\n",
    "- cm[1][1] = number of healthy documents well predicted\n",
    "\n",
    "- cm[0][1] = number of healthy documents predicted as unhealthy\n",
    "\n",
    "- cm[1][0] = number of unhealthy documents predicted as healthy\n",
    "\n",
    "so you can print (cm[0][0]+cm[1][1])/(cm[0][0]+cm[1][1]+cm[1][0]+cm[0][1]) to get the accuracy on the test set !\n",
    "\n",
    "This is a good way to evaluate your final model but __be careful !!!__ we can't use this accuracy to optimize our hyperparameters (number of trees, number of documents, max number of word considerate).!!! If we use this measure of accuracy on the test set to optimize our parameters, the test set become a kind of training set for those parameters and therefore we can no longer use it to give an accurate prediction! In other words, we cannot use our test set before that the model is the good one! the test set should only be used to give the final accuracy of our model!\n",
    "\n",
    "So how to optimize the hyperparameters? We should find a way to measure the accuracy of our model without altering the integrity of our test set! This other way is the cross validation. the cross-validation method allows to measure the accuracy of the model by looking only on the training set. how does it work?\n",
    "- first the training set is splited in 10 (you can modify this value) and each split is divided in a training subset and a test subset\n",
    "- then the algorithm learns on each training subset and calculate his accuracy on the associated test subset\n",
    "- finally, it returns the mean of the 10 accuracy calculate.\n",
    "\n",
    "So, this method allows to evaluate the accuracy without altering the test set; it will allow us to optimize the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8612925797882731"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = classifierRF, X = X_train, y = y_train, cv = 10)\n",
    "accuracies.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "thanks again sklearn! the parameter cv is the number of splits of the training set, 10 is a good value...\n",
    "\n",
    "\n",
    "There is a lot of different parameters we can change to find the optimal model. The most efficient I reach with this algorithm was 0.874, I used the random forest algorithm with 20 trees, the entropy criterion, the size of the data was 10 000 and i was considering the 80 000 most frequent word on a tfidif feature matrix.\n",
    "\n",
    "\n",
    "To be more efficient while looking for the best model you can use the Grid method. This is a sklearn object that test all different method with the cross validation according to the parameters you want it to test and return the best model with the accuracy. here is the code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = [{'n_estimators': [5, 10], 'criterion': ['entropy']}]\n",
    "              \n",
    "grid_search = GridSearchCV(estimator = classifierRF,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 10,\n",
    "                           n_jobs = -1)\n",
    "grid_search = grid_search.fit(X_train, y_train)\n",
    "best_accuracy = grid_search.best_score_\n",
    "best_parameters = grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can look the documentation to get more detail on this object but on a normal computer running this algorithm can take hours..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI- Save the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I put here some quick line to Save a model in your working directory a load it later. It can take time to fit a model especially in our case with such a huge matrix of feature.\n",
    "Don't forget to also download the preprocessing objects!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#directory=\"Models\\model_RF_1.json\"\n",
    "def Save_model (directory,model):\n",
    "    pickle.dump(model, open(directory, 'wb'))\n",
    "    print(\"Saved model to disk\")\n",
    "\n",
    "\n",
    "def Load_model(directory):\n",
    "    loaded_model = pickle.load(open(directory, 'rb'))\n",
    "    print(\"Loaded model from disk\")\n",
    "    return (loaded_model)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
